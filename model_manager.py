"""
model_manager.py - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ YOLO –º–æ–¥–µ–ª—è–º–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA
"""
import logging
import torch
from typing import Optional, Dict, Any
from ultralytics import YOLO
from config import YOLO_MODELS, YOLO_CONFIG, DEVICE_INFO

logger = logging.getLogger(__name__)

class ModelManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è YOLO –º–æ–¥–µ–ª—è–º–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA"""

    def __init__(self):

        self.yolo_seg_model: Optional[YOLO] = None
        self.yolo_det_model: Optional[YOLO] = None
        self.models_loaded = False
        self.device_info = DEVICE_INFO
        self.performance_stats = {
            'inference_times': [],
            'memory_usage': [],
            'total_inferences': 0
        }

    def load_models(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π YOLO —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""

        try:
            logger.info("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π YOLO...")
            self._log_device_info()

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            logger.info(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {YOLO_MODELS['segmentation']}")
            self.yolo_seg_model = YOLO(YOLO_MODELS['segmentation'])
            self._configure_model(self.yolo_seg_model, "—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏
            logger.info(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏: {YOLO_MODELS['detection']}")
            self.yolo_det_model = YOLO(YOLO_MODELS['detection'])
            self._configure_model(self.yolo_det_model, "–¥–µ—Ç–µ–∫—Ü–∏–∏")

            # –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–µ–π
            self._warmup_models()
            self.models_loaded = True
            logger.info("‚úÖ YOLO –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")

            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            self._log_performance_info()
            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ YOLO –º–æ–¥–µ–ª–µ–π: {e}")
            self.models_loaded = False
            return False

    def _log_device_info(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ"""
        logger.info("üîç –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ:")
        logger.info(f"   üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device_info['device'].upper()}")       

        if self.device_info['available']:
            logger.info(f"   üî• GPU: {self.device_info['gpu_name']}")
            logger.info(f"   üíæ GPU –ø–∞–º—è—Ç—å: {self.device_info['gpu_memory_gb']} GB")
            logger.info(f"   üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {self.device_info['gpu_count']}")

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ CUDA
            if torch.cuda.is_available():
                logger.info(f"   üîß CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
                logger.info(f"   ‚ö° cuDNN –≤–µ—Ä—Å–∏—è: {torch.backends.cudnn.version()}")
                logger.info(f"   üéØ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GPU: {torch.cuda.get_device_capability(0)}")
        else:
            logger.warning("   ‚ö†Ô∏è CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
            logger.info("   üí° –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ CUDA –∏ PyTorch —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π")

    def _configure_model(self, model: YOLO, model_type: str):
        """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

        try:
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            model.to(self.device_info['device'])
            logger.info(f"   üìç –ú–æ–¥–µ–ª—å {model_type} –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ {self.device_info['device'].upper()}")
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ precision –¥–ª—è GPU
            if self.device_info['available'] and YOLO_CONFIG['half']:
                try:
                    model.half()  # –ò—Å–ø–æ–ª—å–∑—É–µ–º FP16 –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                    logger.info(f"   üîß –ú–æ–¥–µ–ª—å {model_type} –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞ –≤ —Ä–µ–∂–∏–º FP16")
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å FP16 –¥–ª—è –º–æ–¥–µ–ª–∏ {model_type}: {e}")

            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è inference
            model.eval()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º inference            

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ {model_type}: {e}")
            raise

    def _warmup_models(self):
        """–ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

        try:
            logger.info("üî• –ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–µ–π...")

            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            import numpy as np
            dummy_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)

            # –ü—Ä–æ–≥—Ä–µ–≤–∞–µ–º –º–æ–¥–µ–ª—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            if self.yolo_seg_model:
                with torch.no_grad():
                    _ = self.yolo_seg_model(dummy_image, **YOLO_CONFIG)
                logger.info("   ‚úÖ –ú–æ–¥–µ–ª—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–µ—Ç–∞")

            # –ü—Ä–æ–≥—Ä–µ–≤–∞–µ–º –º–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏
            if self.yolo_det_model:
                with torch.no_grad():
                    _ = self.yolo_det_model(dummy_image, **YOLO_CONFIG)
                logger.info("   ‚úÖ –ú–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø—Ä–æ–≥—Ä–µ—Ç–∞")

            # –û—á–∏—â–∞–µ–º –∫—ç—à GPU
            if self.device_info['available']:
                torch.cuda.empty_cache()
                logger.info("   üßπ GPU –∫—ç—à –æ—á–∏—â–µ–Ω") 

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≥—Ä–µ–≤–∞ –º–æ–¥–µ–ª–µ–π: {e}")

    def _log_performance_info(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

        if self.device_info['available']:
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ GPU –ø–∞–º—è—Ç–∏
            allocated = torch.cuda.memory_allocated(0) / 1024**2  # MB
            cached = torch.cuda.memory_reserved(0) / 1024**2  # MB
            logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ GPU –ø–∞–º—è—Ç–∏:")
            logger.info(f"   üì¶ –í—ã–¥–µ–ª–µ–Ω–æ: {allocated:.1f} MB")
            logger.info(f"   üíæ –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {cached:.1f} MB")
            logger.info(f"   üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {(allocated/cached)*100:.1f}%" if cached > 0 else "   üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: N/A")

    def get_segmentation_model(self) -> Optional[YOLO]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        return self.yolo_seg_model if self.models_loaded else None

    def get_detection_model(self) -> Optional[YOLO]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        return self.yolo_det_model if self.models_loaded else None

    def are_models_loaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ª–∏ –º–æ–¥–µ–ª–∏"""
        return self.models_loaded

    def predict_with_stats(self, model: YOLO, image, model_name: str = "unknown"):
        """Inference —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not model or not self.models_loaded:
            return None

        import time

        start_time = time.time()

        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º inference
            with torch.no_grad():
                results = model(image, **YOLO_CONFIG)
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            inference_time = time.time() - start_time
            self.performance_stats['inference_times'].append(inference_time)
            self.performance_stats['total_inferences'] += 1

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
            if len(self.performance_stats['inference_times']) > 100:
                self.performance_stats['inference_times'].pop(0)

            # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 100 inference
            if self.performance_stats['total_inferences'] % 100 == 0:
                avg_time = sum(self.performance_stats['inference_times'][-50:]) / min(50, len(self.performance_stats['inference_times']))
                fps = 1.0 / avg_time if avg_time > 0 else 0
                logger.info(f"üìä {model_name}: {self.performance_stats['total_inferences']} inference, "
                           f"avg time: {avg_time*1000:.1f}ms, FPS: {fps:.1f}")

            return results

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ inference {model_name}: {e}")
            return None

    def get_performance_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

        if not self.performance_stats['inference_times']:
            return {
                'total_inferences': 0,
                'average_time_ms': 0,
                'fps': 0,
                'device': self.device_info['device']
            }
    
        recent_times = self.performance_stats['inference_times'][-50:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 50
        avg_time = sum(recent_times) / len(recent_times)
        fps = 1.0 / avg_time if avg_time > 0 else 0

        stats = {
            'total_inferences': self.performance_stats['total_inferences'],
            'average_time_ms': round(avg_time * 1000, 1),
            'fps': round(fps, 1),
            'device': self.device_info['device'],
            'device_info': self.device_info
        }

        # –î–æ–±–∞–≤–ª—è–µ–º GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if self.device_info['available']:
            stats.update({
                'gpu_memory_allocated_mb': round(torch.cuda.memory_allocated(0) / 1024**2, 1),
                'gpu_memory_cached_mb': round(torch.cuda.memory_reserved(0) / 1024**2, 1),
                'gpu_utilization': self._get_gpu_utilization()
            })

        return stats

    def _get_gpu_utilization(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —É—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ GPU (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)"""

        try:
            # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞–º—è—Ç–∏
            allocated = torch.cuda.memory_allocated(0)
            total = torch.cuda.get_device_properties(0).total_memory
            return round((allocated / total) * 100, 1)
        except:
            return 0.0
    
    def unload_models(self):
        """–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –∏–∑ –ø–∞–º—è—Ç–∏"""

        try:
            if self.yolo_seg_model:
                del self.yolo_seg_model
                self.yolo_seg_model = None
                logger.info("üóëÔ∏è –ú–æ–¥–µ–ª—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –≤—ã–≥—Ä—É–∂–µ–Ω–∞")

            if self.yolo_det_model:
                del self.yolo_det_model
                self.yolo_det_model = None
                logger.info("üóëÔ∏è –ú–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏ –≤—ã–≥—Ä—É–∂–µ–Ω–∞")

            # –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å
            if self.device_info['available']:
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                logger.info("üßπ GPU –ø–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞")

            self.models_loaded = False
            logger.info("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –≤—ã–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –ø–∞–º—è—Ç–∏")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")

    def get_model_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö"""

        info = {
            'segmentation_model': YOLO_MODELS['segmentation'],
            'detection_model': YOLO_MODELS['detection'],
            'models_loaded': self.models_loaded,
            'segmentation_loaded': self.yolo_seg_model is not None,
            'detection_loaded': self.yolo_det_model is not None,
            'device_info': self.device_info,
            'yolo_config': YOLO_CONFIG,
            'performance_stats': self.get_performance_stats()
        }

        return info

    def optimize_memory(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""

        try:
            if self.device_info['available']:
                # –û—á–∏—â–∞–µ–º –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –ø–∞–º—è—Ç—å
                torch.cuda.empty_cache()

                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
                import gc
                gc.collect()

                logger.info("üîß –ü–∞–º—è—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏: {e}")
