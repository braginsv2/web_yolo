"""
camera_processor.py - –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–æ–≤ –∫–∞–º–µ—Ä —Å –ø–æ–¥—Å—á–µ—Ç–æ–º –ø–ª–æ—â–∞–¥–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
"""

import cv2
import numpy as np
import threading
import time
import logging
import queue
from typing import Optional, Callable

from config import CAMERA_CONFIG, PROCESSING_CONFIG, OBJECT_CLASSES

logger = logging.getLogger(__name__)

class CameraProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞ –∫–∞–º–µ—Ä—ã —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA –∏ –ø–æ–¥—Å—á–µ—Ç–æ–º –ø–ª–æ—â–∞–¥–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
    
    def __init__(self, camera_id: str, camera_streams: dict, yolo_model, alarm_callback: Callable, model_manager=None, segmentation_callback=None):
        self.camera_id = camera_id
        self.camera_streams = camera_streams
        self.yolo_model = yolo_model
        self.alarm_callback = alarm_callback
        self.model_manager = model_manager
        self.segmentation_callback = segmentation_callback  # –ù–æ–≤—ã–π callback –¥–ª—è –ø–ª–æ—â–∞–¥–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        
        self.running = False
        self.capture_thread: Optional[threading.Thread] = None
        self.process_thread: Optional[threading.Thread] = None
        self.lock = threading.Lock()
        self.process_queue = queue.Queue(maxsize=PROCESSING_CONFIG['queue_maxsize'])
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.frame_stats = {
            'processed_frames': 0,
            'dropped_frames': 0,
            'last_fps_update': time.time(),
            'fps': 0.0
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.segmentation_stats = {
            'last_segmentation_area': 0,
            'total_segmentation_pixels': 0,
            'frames_with_segmentation': 0,
            'average_segmentation_area': 0.0
        }

    def connect_camera(self, rtsp_url: str) -> bool:
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ RTSP –∫–∞–º–µ—Ä–µ"""
        try:
            logger.info(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ RTSP –∫–∞–º–µ—Ä–µ: {rtsp_url}")
            
            cap = cv2.VideoCapture(rtsp_url, cv2.CAP_FFMPEG)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è RTSP
            self._configure_camera(cap)
            
            if cap.isOpened():
                success = self._test_camera_connection(cap)
                if success:
                    with self.lock:
                        self.camera_streams[self.camera_id]['cap'] = cap
                        self.camera_streams[self.camera_id]['connected'] = True
                        self.camera_streams[self.camera_id]['config'] = {'rtsp_url': rtsp_url}
                        self.camera_streams[self.camera_id]['frame_counter'] = 0
                    
                    logger.info(f"–ö–∞–º–µ—Ä–∞ {self.camera_id} –ø–æ–¥–∫–ª—é—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                    return True
                else:
                    cap.release()
                    return False
            else:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∫–∞–º–µ—Ä—É {self.camera_id}")
                return False
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –∫–∞–º–µ—Ä–µ {self.camera_id}: {e}")
            return False

    def _configure_camera(self, cap):
        """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–∞–º–µ—Ä—ã"""
        cap.set(cv2.CAP_PROP_BUFFERSIZE, CAMERA_CONFIG['buffer_size'])
        cap.set(cv2.CAP_PROP_FPS, CAMERA_CONFIG['fps'])
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, CAMERA_CONFIG['width'])
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, CAMERA_CONFIG['height'])
        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('H', '2', '6', '4'))
        cap.set(cv2.CAP_PROP_OPEN_TIMEOUT_MSEC, CAMERA_CONFIG['open_timeout'])
        cap.set(cv2.CAP_PROP_READ_TIMEOUT_MSEC, CAMERA_CONFIG['read_timeout'])

    def _test_camera_connection(self, cap) -> bool:
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –∫–∞–º–µ—Ä–µ"""
        for attempt in range(CAMERA_CONFIG['max_attempts']):
            ret, frame = cap.read()
            if ret and frame is not None:
                logger.info(f"–ö–∞–¥—Ä –ø–æ–ª—É—á–µ–Ω —É—Å–ø–µ—à–Ω–æ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}")
                return True
            else:
                time.sleep(1)
        
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä –ø–æ—Å–ª–µ {CAMERA_CONFIG['max_attempts']} –ø–æ–ø—ã—Ç–æ–∫")
        return False

    def disconnect_camera(self):
        """–û—Ç–∫–ª—é—á–µ–Ω–∏–µ –∫–∞–º–µ—Ä—ã"""
        self.running = False
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤
        if self.capture_thread and self.capture_thread.is_alive():
            self.capture_thread.join(timeout=3)
        if self.process_thread and self.process_thread.is_alive():
            self.process_thread.join(timeout=3)
            
        with self.lock:
            if self.camera_streams[self.camera_id]['cap']:
                self.camera_streams[self.camera_id]['cap'].release()
                
            # –°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–∞–º–µ—Ä—ã
            self.camera_streams[self.camera_id].update({
                'cap': None,
                'connected': False,
                'frame': None,
                'processed_frame': None,
                'processing': False,
                'frame_counter': 0,
                'segmentation_area': 0  # –°–±—Ä–æ—Å –ø–ª–æ—â–∞–¥–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            })
            
            if self.camera_streams[self.camera_id]['frame_queue']:
                self.camera_streams[self.camera_id]['frame_queue'].clear()
            
        logger.info(f"–ö–∞–º–µ—Ä–∞ {self.camera_id} –æ—Ç–∫–ª—é—á–µ–Ω–∞")

    def start_processing(self) -> bool:
        """–ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–∞"""
        if not self.camera_streams[self.camera_id]['connected']:
            return False
            
        self.running = True
        self.camera_streams[self.camera_id]['processing'] = True
        
        self.capture_thread = threading.Thread(target=self._capture_loop, daemon=True)
        self.process_thread = threading.Thread(target=self._process_loop, daemon=True)
        
        self.capture_thread.start()
        self.process_thread.start()
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–º–µ—Ä—ã {self.camera_id} –∑–∞–ø—É—â–µ–Ω–∞")
        return True

    def _capture_loop(self):
        """–ü–æ—Ç–æ–∫ –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –∫–∞–¥—Ä–æ–≤"""
        logger.info(f"–ó–∞–ø—É—â–µ–Ω –ø–æ—Ç–æ–∫ –∑–∞—Ö–≤–∞—Ç–∞ –¥–ª—è –∫–∞–º–µ—Ä—ã {self.camera_id}")
        
        while self.running:
            try:
                with self.lock:
                    cap = self.camera_streams[self.camera_id]['cap']
                    if not cap or not cap.isOpened():
                        break
                
                ret, frame = cap.read()
                if not ret:
                    time.sleep(0.01)
                    continue
                
                # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞
                frame = cv2.resize(frame, (CAMERA_CONFIG['width'], CAMERA_CONFIG['height']))
                
                with self.lock:
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å –∫–∞–¥—Ä–æ–≤
                    if self.camera_streams[self.camera_id]['frame_queue']:
                        self.camera_streams[self.camera_id]['frame_queue'].append(frame.copy())
                    
                    self.camera_streams[self.camera_id]['frame'] = frame.copy()
                    self.camera_streams[self.camera_id]['frame_counter'] += 1
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–¥—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä)
                if self.camera_streams[self.camera_id]['frame_counter'] % PROCESSING_CONFIG['frame_skip'] == 0:
                    try:
                        self.process_queue.put_nowait(frame.copy())
                    except queue.Full:
                        # –°—á–∏—Ç–∞–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∫–∞–¥—Ä—ã
                        self.frame_stats['dropped_frames'] += 1
                
                time.sleep(0.01)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ –∑–∞—Ö–≤–∞—Ç–∞ {self.camera_id}: {e}")
                break
        
        logger.info(f"–ü–æ—Ç–æ–∫ –∑–∞—Ö–≤–∞—Ç–∞ –¥–ª—è –∫–∞–º–µ—Ä—ã {self.camera_id} –∑–∞–≤–µ—Ä—à–µ–Ω")

    def _process_loop(self):
        """–ü–æ—Ç–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–¥—Ä–æ–≤ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        logger.info(f"–ó–∞–ø—É—â–µ–Ω –ø–æ—Ç–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –∫–∞–º–µ—Ä—ã {self.camera_id}")
        
        while self.running:
            try:
                try:
                    frame = self.process_queue.get(timeout=1.0)
                except queue.Empty:
                    continue
                
                # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ FPS
                current_time = time.time()
                self.frame_stats['processed_frames'] += 1
                
                # –û–±–Ω–æ–≤–ª—è–µ–º FPS –∫–∞–∂–¥—ã–µ 30 –∫–∞–¥—Ä–æ–≤
                if self.frame_stats['processed_frames'] % 30 == 0:
                    time_diff = current_time - self.frame_stats['last_fps_update']
                    if time_diff > 0:
                        self.frame_stats['fps'] = 30 / time_diff
                        self.frame_stats['last_fps_update'] = current_time
                
                # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–∞–º–µ—Ä—ã
                if self.camera_id == 'camera1':
                    processed_frame = self._process_frame_segmentation(frame)
                else:
                    processed_frame = self._process_frame_detection(frame)
                
                with self.lock:
                    self.camera_streams[self.camera_id]['processed_frame'] = processed_frame
                
                self.process_queue.task_done()
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥—ã–µ 1000 –∫–∞–¥—Ä–æ–≤
                if self.frame_stats['processed_frames'] % 1000 == 0:
                    logger.info(f"üìä {self.camera_id}: {self.frame_stats['processed_frames']} –∫–∞–¥—Ä–æ–≤, "
                              f"FPS: {self.frame_stats['fps']:.1f}, "
                              f"–ø—Ä–æ–ø—É—â–µ–Ω–æ: {self.frame_stats['dropped_frames']}, "
                              f"—Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {self.segmentation_stats['last_segmentation_area']} –ø–∏–∫—Å.")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–æ—Ç–æ–∫–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {self.camera_id}: {e}")
                break
        
        self.camera_streams[self.camera_id]['processing'] = False
        logger.info(f"–ü–æ—Ç–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –∫–∞–º–µ—Ä—ã {self.camera_id} –∑–∞–≤–µ—Ä—à–µ–Ω")

    def get_performance_stats(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–º–µ—Ä—ã"""
        return {
            'camera_id': self.camera_id,
            'processed_frames': self.frame_stats['processed_frames'],
            'dropped_frames': self.frame_stats['dropped_frames'],
            'fps': round(self.frame_stats['fps'], 1),
            'queue_size': self.process_queue.qsize(),
            'is_running': self.running,
            'segmentation_area': self.segmentation_stats['last_segmentation_area'],
            'avg_segmentation_area': round(self.segmentation_stats['average_segmentation_area'], 1),
            'frames_with_segmentation': self.segmentation_stats['frames_with_segmentation']
        }

    def _calculate_segmentation_area(self, masks, boxes) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –ø–ª–æ—â–∞–¥–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ª—é–¥–µ–π –≤ –ø–∏–∫—Å–µ–ª—è—Ö"""
        total_area = 0
        
        if masks is not None and boxes is not None:
            masks_data = masks.data.cpu().numpy()
            
            for i, (mask, box) in enumerate(zip(masks_data, boxes)):
                cls = int(box.cls[0].cpu().numpy())
                if cls == OBJECT_CLASSES['person']:
                    # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –º–∞—Å–∫–∏ –ø–æ–¥ —Ä–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞
                    mask_resized = cv2.resize(mask, (CAMERA_CONFIG['width'], CAMERA_CONFIG['height']))
                    
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∏–∫—Å–µ–ª–∏ –º–∞—Å–∫–∏ (–≥–¥–µ –∑–Ω–∞—á–µ–Ω–∏–µ > 0.5)
                    person_pixels = np.sum(mask_resized > 0.5)
                    total_area += person_pixels
        
        return int(total_area)

    def _process_frame_segmentation(self, frame):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (–∫–∞–º–µ—Ä–∞ 1)"""
        if not self.yolo_model:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø—Ä–æ—Å—Ç–æ –æ–±–Ω—É–ª—è–µ–º –ø–ª–æ—â–∞–¥—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            self._update_segmentation_stats(0)
            return frame
            
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º model_manager –¥–ª—è inference —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if hasattr(self, 'model_manager') and self.model_manager:
                results = self.model_manager.predict_with_stats(
                    self.yolo_model, frame, "–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è"
                )
            else:
                # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π inference
                results = self.yolo_model(frame, verbose=False)
            
            if not results:
                # –û–±–Ω—É–ª—è–µ–º –ø–ª–æ—â–∞–¥—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                self._update_segmentation_stats(0)
                return frame
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–ª–æ—â–∞–¥—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            segmentation_area = 0
            if results[0].masks is not None and results[0].boxes is not None:
                segmentation_area = self._calculate_segmentation_area(results[0].masks, results[0].boxes)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            self._update_segmentation_stats(segmentation_area)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ª—é–¥–µ–π
            person_detected = self._check_person_detection(results)
            
            # –°–æ–∑–¥–∞–µ–º –∞–ª–∞—Ä–º –µ—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω —á–µ–ª–æ–≤–µ–∫
            if person_detected:
                self.alarm_callback(self.camera_id, frame)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–∞–¥—Ä —Ç–æ–ª—å–∫–æ —Å –º–∞—Å–∫–∞–º–∏ –ª—é–¥–µ–π
            return self._draw_segmentation_masks(frame, results)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–¥—Ä–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ {self.camera_id}: {e}")
            self._update_segmentation_stats(0)
            return frame

    def _process_frame_detection(self, frame):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ (–∫–∞–º–µ—Ä–∞ 2)"""
        if not self.yolo_model:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø—Ä–æ—Å—Ç–æ –æ–±–Ω—É–ª—è–µ–º –ø–ª–æ—â–∞–¥—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            self._update_segmentation_stats(0)
            return frame
            
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º model_manager –¥–ª—è inference —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if hasattr(self, 'model_manager') and self.model_manager:
                results = self.model_manager.predict_with_stats(
                    self.yolo_model, frame, "–î–µ—Ç–µ–∫—Ü–∏—è"
                )
            else:
                # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π inference
                results = self.yolo_model(frame, verbose=False)
            
            if not results:
                # –û–±–Ω—É–ª—è–µ–º –ø–ª–æ—â–∞–¥—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                self._update_segmentation_stats(0)
                return frame
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–ª–æ—â–∞–¥—å —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –º–∞—Å–∫–∏)
            segmentation_area = 0
            if results[0].masks is not None and results[0].boxes is not None:
                segmentation_area = self._calculate_segmentation_area(results[0].masks, results[0].boxes)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            self._update_segmentation_stats(segmentation_area)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ª—é–¥–µ–π
            person_detected = self._check_person_detection(results)
            
            # –°–æ–∑–¥–∞–µ–º –∞–ª–∞—Ä–º –µ—Å–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω —á–µ–ª–æ–≤–µ–∫
            if person_detected:
                self.alarm_callback(self.camera_id, frame)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –º–∞—Å–∫–∞–º–∏ –∏ –±–æ–∫—Å–∞–º–∏ –¥–ª—è –ª—é–¥–µ–π
            annotated_frame = self._draw_segmentation_masks(frame, results)
            return self._draw_detection_boxes(annotated_frame, results)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–¥—Ä–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ {self.camera_id}: {e}")
            self._update_segmentation_stats(0)
            return frame

    def _update_segmentation_stats(self, area: int):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        self.segmentation_stats['last_segmentation_area'] = area
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ camera_streams –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∏–∑–≤–Ω–µ
        with self.lock:
            self.camera_streams[self.camera_id]['segmentation_area'] = area
        
        if area > 0:
            self.segmentation_stats['frames_with_segmentation'] += 1
            self.segmentation_stats['total_segmentation_pixels'] += area
            
            # –ü–æ–¥—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–π –ø–ª–æ—â–∞–¥–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            self.segmentation_stats['average_segmentation_area'] = (
                self.segmentation_stats['total_segmentation_pixels'] / 
                self.segmentation_stats['frames_with_segmentation']
            )
        
        # –í—ã–∑—ã–≤–∞–µ–º callback –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ø–ª–æ—â–∞–¥–µ–π
        if self.segmentation_callback:
            try:
                self.segmentation_callback(self.camera_id, area)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ segmentation_callback: {e}")

    def _check_person_detection(self, results) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ª—é–¥–µ–π –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        try:
            if results and len(results) > 0 and results[0].boxes is not None:
                for box in results[0].boxes:
                    cls = int(box.cls[0].cpu().numpy())
                    if cls == OBJECT_CLASSES['person']:
                        return True
            return False
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª—é–¥–µ–π: {e}")
            return False

    def _draw_segmentation_masks(self, frame, results):
        """–†–∏—Å–æ–≤–∞–Ω–∏–µ –º–∞—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –ª—é–¥–µ–π"""
        try:
            annotated_frame = frame.copy()
            
            if (results and len(results) > 0 and 
                results[0].masks is not None and results[0].boxes is not None):
                
                masks = results[0].masks.data.cpu().numpy()
                boxes = results[0].boxes
                
                for i, (mask, box) in enumerate(zip(masks, boxes)):
                    cls = int(box.cls[0].cpu().numpy())
                    if cls == OBJECT_CLASSES['person']:
                        # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –º–∞—Å–∫–∏ –ø–æ–¥ —Ä–∞–∑–º–µ—Ä –∫–∞–¥—Ä–∞
                        mask_resized = cv2.resize(mask, (frame.shape[1], frame.shape[0]))
                        
                        # –°–æ–∑–¥–∞–µ–º —Ü–≤–µ—Ç–Ω—É—é –º–∞—Å–∫—É –¥–ª—è —á–µ–ª–æ–≤–µ–∫–∞
                        color = np.random.randint(50, 255, 3)
                        colored_mask = np.zeros_like(frame)
                        colored_mask[mask_resized > 0.5] = color
                        
                        # –ù–∞–∫–ª–∞–¥—ã–≤–∞–µ–º –º–∞—Å–∫—É —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å—é
                        alpha = 0.7 if self.camera_id == 'camera1' else 0.8
                        beta = 0.3 if self.camera_id == 'camera1' else 0.2
                        annotated_frame = cv2.addWeighted(annotated_frame, alpha, colored_mask, beta, 0)
            
            return annotated_frame
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∏—Å–æ–≤–∞–Ω–∏—è –º–∞—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {e}")
            return frame

    def _draw_detection_boxes(self, frame, results):
        """–†–∏—Å–æ–≤–∞–Ω–∏–µ –±–æ–∫—Å–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–ª—è –ª—é–¥–µ–π"""
        try:
            if results and len(results) > 0 and results[0].boxes is not None:
                boxes = results[0].boxes
                for box in boxes:
                    cls = int(box.cls[0].cpu().numpy())
                    if cls == OBJECT_CLASSES['person']:
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
                        conf = box.conf[0].cpu().numpy()
                        
                        # –†–∏—Å—É–µ–º –±–æ–∫—Å –¥–ª—è —á–µ–ª–æ–≤–µ–∫–∞
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º label —Å confidence
                        label = f"Person: {conf:.2f}"
                        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                        
                        # –§–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞
                        cv2.rectangle(frame, (x1, y1-30), (x1 + label_size[0], y1), (0, 255, 0), -1)
                        cv2.putText(frame, label, (x1, y1-10), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)
            
            return frame
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∏—Å–æ–≤–∞–Ω–∏—è –±–æ–∫—Å–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏: {e}")
            return frame


class VideoStreamGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞ –¥–ª—è Flask"""
    
    def __init__(self, camera_streams: dict):
        self.camera_streams = camera_streams

    def generate_frames(self, camera_id: str, processed: bool = True):
        """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–∞–¥—Ä–æ–≤ –¥–ª—è —Å—Ç—Ä–∏–º–∞"""
        while True:
            try:
                if processed:
                    frame = self.camera_streams[camera_id]['processed_frame']
                else:
                    frame = self.camera_streams[camera_id]['frame']
                    
                if frame is not None:
                    ret, buffer = cv2.imencode('.jpg', frame, [
                        cv2.IMWRITE_JPEG_QUALITY, PROCESSING_CONFIG['jpeg_quality'],
                        cv2.IMWRITE_JPEG_OPTIMIZE, 1
                    ])
                    if ret:
                        frame_bytes = buffer.tobytes()
                        yield (b'--frame\r\n'
                               b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
                else:
                    time.sleep(0.05)
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞–¥—Ä–æ–≤ –¥–ª—è {camera_id}: {e}")
                time.sleep(0.1)


class SegmentationAreaManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ø–ª–æ—â–∞–¥–µ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–≤—É—Ö –∫–∞–º–µ—Ä"""
    
    def __init__(self):
        self.camera_areas = {
            'camera1': 0,
            'camera2': 0
        }
        self.area_product = 0
        self.lock = threading.Lock()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'max_product': 0,
            'total_calculations': 0,
            'average_product': 0.0,
            'non_zero_products': 0
        }
    
    def update_camera_area(self, camera_id: str, area: int):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –∫–∞–º–µ—Ä—ã"""
        with self.lock:
            self.camera_areas[camera_id] = area
            self._calculate_product()
    
    def _calculate_product(self):
        """–ü–æ–¥—Å—á–µ—Ç –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ø–ª–æ—â–∞–¥–µ–π"""
        new_product = self.camera_areas['camera1'] * self.camera_areas['camera2']
        self.area_product = new_product
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.stats['total_calculations'] += 1
        
        if new_product > self.stats['max_product']:
            self.stats['max_product'] = new_product
        
        if new_product > 0:
            self.stats['non_zero_products'] += 1
            
        # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç (—Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)
        if self.stats['non_zero_products'] > 0:
            total_sum = self.stats['average_product'] * (self.stats['non_zero_products'] - 1) + new_product
            self.stats['average_product'] = total_sum / self.stats['non_zero_products']
    
    def get_current_product(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –ø–ª–æ—â–∞–¥–µ–π"""
        with self.lock:
            return self.area_product
    
    def get_camera_areas(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–µ–π –ø–æ –∫–∞–º–µ—Ä–∞–º"""
        with self.lock:
            return self.camera_areas.copy()
    
    def get_stats(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        with self.lock:
            return {
                'current_product': self.area_product,
                'camera1_area': self.camera_areas['camera1'],
                'camera2_area': self.camera_areas['camera2'],
                'max_product': self.stats['max_product'],
                'average_product': round(self.stats['average_product'], 1),
                'total_calculations': self.stats['total_calculations'],
                'non_zero_products': self.stats['non_zero_products']
            }